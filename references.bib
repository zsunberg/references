@String { aaai        = {AAAI Conference on Artificial Intelligence (AAAI)} }
@String { aamas       = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)} }
@String { acc         = {American Control Conference (ACC)} }
@String { atm         = {Air Traffic Management Research and Development Seminar}}
@String { aiaa_info   = {AIAA Infotech@Aerospace Conference} }
@String { aiaa_jacic  = {Journal of Aerospace Computing, Information, and Communication} }
@String { allerton    = {Allerton Conference on Communication, Control, and Compution} }
@String { atio        = {AIAA Aviation Technology, Integration, and Operations Conference (ATIO)} }
@String { cacm        = {Communications of the ACM} }
@String { cav         = {International Conference on Computer-Aided Verification} }
@String { cdc         = {IEEE Conference on Decision and Control (CDC)} }
@String { cvpr        = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)} }
@String { dasc        = {Digital Avionics Systems Conference (DASC)} }
@String { ecml        = {European Conference on Machine Learning (ECML)} }
@String { gnc         = {AIAA Guidance, Navigation, and Control Conference (GNC)} }
@String { icaart      = {International Conference on Agents and Artificial Intelligence (ICAART)} }
@String { icaps       = {International Conference on Automated Planning and Scheduling (ICAPS)} }
@String { icassp      = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)} }
@String { iclr        = {International Conference on Learning Representations (ICLR)} }
@String { icml        = {International Conference on Machine Learning (ICML)} }
@String { icmla       = {International Conference on Machine Learning and Applications (ICMLA)} }
@String { icra        = {IEEE International Conference on Robotics and Automation (ICRA)} }
@String { icslp       = {International Conference on Spoken Language Processing (ICSLP)} }
@String { ieee_csm    = {IEEE Control Systems Magazine} }
@String { ieee_j_ac   = {IEEE Transactions on Automatic Control} }
@String { ieeeaero    = {IEEE Aerospace Conference} }
@String { ieeeciaig   = {IEEE Transactions on Computational Intelligence and AI in Games} }
@String { ieeecst     = {IEEE Transactions on Control Systems Technology} }
@String { ieeetiv     = {IEEE Transactions on Intelligent Vehicles} }
@String { ieeetac     = {IEEE Transactions on Automatic Control} }
@String { ieeetits     = {IEEE Transactions on Intelligent Transportation Systems} }
@String { ieeetsp     = {IEEE Transactions on Signal Processing} }
@String { ijcai       = {International Joint Conference on Artificial Intelligence (IJCAI)} }
@String { ijrr        = {International Journal of Robotics Research} }
@String { interspeech = {Annual Conference of the International Speech Communication Association (INTERSPEECH)} }
@String { iros        = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)} }
@String { itsc        = {IEEE International Conference on Intelligent Transportation Systems (ITSC)} }
@String { iv          = {IEEE Intelligent Vehicles Symposium (IV)} }
@String { jaamas      = {Journal of Autonomous Agents and Multi-Agent Systems} }
@String { jair        = {Journal of Artificial Intelligence Research} }
@String { jgcd        = {AIAA Journal of Guidance, Control, and Dynamics} }
@String { jmlr        = {Journal of Machine Learning Research} }
@String { jota        = {Journal of Optimization Theory and Applications} }
@String { lion        = {Learning and Intelligent Optimization (LION)} }
@String { mor         = {Mathematics of Operations Research} }
@String { nips        = {Advances in Neural Information Processing Systems (NIPS)} }
@String { neurips     = {Advances in Neural Information Processing Systems (NeurIPS)} }
@String { or          = {Operations Research} }
@String { rss         = {Robotics: Science and Systems} }
@String { sigcomm     = {ACM Special Interest Group on Data Communication (SIGCOMM)} }
@String { tac         = {IEEE Transactions on Automatic Control} }
@String { tacas       = {International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS)} }
@String { taes        = {IEEE Transactions on Aerospace and Electronic Systems} }
@String { uai         = {Conference on Uncertainty in Artificial Intelligence (UAI)} }

@inproceedings{sunberg2018pomcpow,
  title={Online Algorithms for {POMDP}s with Continuous State, Action, and Observation Spaces},
  author={Zachary Sunberg and Mykel J. Kochenderfer},
  booktitle={International Conference on Automated Planning and Scheduling},
  year={2018}
}

@inproceedings{garg2019despotalpha,
  title={{DESPOT}-$\alpha$: Online {POMDP} Planning With Large State And Observation Spaces},
  author={Neha P. Garg and David Hsu and Wee Sun Lee},
  booktitle={Robotics: Science and Systems},
  year={2019}
}

@inproceedings{lee2020monte,
  title={{M}onte-{C}arlo Tree Search in Continuous Action Spaces with Value Gradients},
  author={Lee, Jongmin and Jeon, Wonseok and Kim, Geon-Hyeong and Kim, Kee-Eung},
  booktitle=aaai,
  pages={4561--4568},
  year={2020}
}

@inproceedings{kim2020monte,
  title={Monte Carlo Tree Search in Continuous Spaces Using Voronoi Optimistic Optimization with Regret Bounds.},
  author={Kim, Beomjoon and Lee, Kyungjae and Lim, Sungbin and Kaelbling, Leslie Pack and Lozano-P{\'e}rez, Tom{\'a}s},
  booktitle=aaai,
  pages={9916--9924},
  year={2020}
}

@inproceedings{lim2020sparse,
  title={Sparse Tree Search Optimality Guarantees in {POMDP}s with Continuous Observation Spaces},
  author={Lim, Michael H. and Tomlin, Claire J. and Sunberg, Zachary N.},
  year={2020},
  booktitle=ijcai
}

@inproceedings{garg2019learning,
  title={Learning To Grasp Under Uncertainty Using POMDPs},
  author={Garg, Neha P and Hsu, David and Lee, Wee Sun},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={2751--2757},
  year={2019},
  organization={IEEE}
}

@article{vinyals2019grandmaster,
  title={Grandmaster Level in {S}tarCraft {II} using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@misc{darpa2020ace,
    title={Broad Agency Announcement: Air Combat Evolution Technical Area 1: Build Combat Autonomy},
    author={Defence Advanced Research Projects Agency (DARPA)},
    year={2020}
}

@article {jaderberg2019quake,
	author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Casta{\~n}eda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
	title = {Human-level performance in 3D multiplayer games with population-based reinforcement learning},
	volume = {364},
	number = {6443},
	pages = {859--865},
	year = {2019},
	doi = {10.1126/science.aau6249},
	publisher = {American Association for the Advancement of Science},
	abstract = {Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans.Science, this issue p. 859Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/364/6443/859},
	eprint = {https://science.sciencemag.org/content/364/6443/859.full.pdf},
	journal = {Science}
}

@Article{kearns2002sparse,
    author="Kearns, Michael and Mansour, Yishay and Ng, Andrew Y.",
    title="A Sparse Sampling Algorithm for Near-Optimal Planning in Large {M}arkov Decision Processes",
    journal="Machine Learning",
    year="2002",
    month="Nov",
    day="01",
    volume="49",
    number="2",
    pages="193--208",
    issn="1573-0565",
}

@article {silver2018alphago,
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	title = {A general reinforcement learning algorithm that masters {C}hess, {S}hogi, and {G}o through self-play},
	volume = {362},
	number = {6419},
	pages = {1140--1144},
	year = {2018},
	doi = {10.1126/science.aar6404},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.Science, this issue p. 1140; see also pp. 1087 and 1118The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/362/6419/1140},
	eprint = {https://science.sciencemag.org/content/362/6419/1140.full.pdf},
	journal = {Science}
}

@inproceedings{silver2010pomcp,
    title = {{M}onte-{C}arlo Planning in Large {POMDP}s},
    author = {Silver, David and Veness, Joel},
    booktitle = {Advances in Neural Information Processing Systems},
    pages = {2164--2172},
    year = {2010},
    url = {http://papers.nips.cc/paper/4031-monte-carlo-planning-in-large-pomdps.pdf}
}

@article{ye2017despot,
  title={{DESPOT}: Online {POMDP} Planning with Regularization},
  author={Ye, Nan and Somani, Adhiraj and Hsu, David and Lee, Wee Sun},
  journal={Journal of Artificial Intelligence Research},
  volume={58},
  pages={231--266},
  year={2017}
}

@inproceedings{seiler2015online,
  title={An online and approximate solver for {POMDP}s with continuous action space},
  author={Seiler, Konstantin M. and Kurniawati, Hanna and Singh, Surya P. N.},
  booktitle=icra,
  pages={2290--2297},
  year={2015},
}

@article{browne2012survey,
  title={A survey of {M}onte {C}arlo tree search methods},
  author={Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={{IEEE} Transactions on Computational Intelligence and {AI} in games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
}

@inproceedings{keller2013trial,
  title={Trial-based heuristic tree search for finite horizon MDPs},
  author={Keller, Thomas and Helmert, Malte},
  booktitle={Twenty-Third International Conference on Automated Planning and Scheduling},
  year={2013}
}

@inproceedings{sunberg2017value,
    title={The Value of Inferring the Internal State of Traffic Participants for Autonomous Freeway Driving},
    author={Sunberg, Zachary N. and Ho, Christopher J. and Kochenderfer, Mykel, J.},
    booktitle=acc,
    year={2017}
}

@inproceedings{couetoux2011double,
  address = {Rome, Italy},
  annote = {double progressive widening},
  author = {Cou\"{e}toux, A. and Hoock, J.-B. and Sokolovska, N. and Teytaud, O. and Bonnard, N.},
  booktitle = {Learning and Intelligent Optimization},
  mendeley-groups = {UAVCAS,POMDPs,MDPs},
  title = {Continuous Upper Confidence Trees},
  year = {2011}
}

@inproceedings{frank2007hover,
  title={Hover, transition, and level flight control design for a single-propeller indoor airplane},
  author={Frank, Adrian and McGrew, James and Valenti, Mario and Levine, Daniel and How, Jonathan},
  booktitle={AIAA Guidance, Navigation and Control Conference and Exhibit},
  pages={6318},
  year={2007}
}

@book{shaw1985fighter,
  title={Fighter Combat: Tactics and Maneuvering},
  author={Shaw, Robert L.},
  year={1985},
  publisher={Naval Institute Press},
  address={Annapolis}
}

@Article{holland2013optimizing,
  Title                    = {Optimizing the Next Generation Collision Avoidance System for Safe, Suitable, and Acceptable Operational Performance},
  Author                   = {Jessica E. Holland and Mykel J. Kochenderfer and Wesley A. Olson},
  Journal                  = {{A}ir {T}raffic {C}ontrol {Q}uarterly},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {275-297},
  Volume                   = {21}
}

@inproceedings{cassandra1998survey,
  title={A survey of {POMDP} applications},
  author={Cassandra, Anthony R},
  booktitle={Working notes of {AAAI} 1998 fall symposium on planning with partially observable {M}arkov decision processes},
  year={1998}
}

@article{ayer2012mammography,
  title={A {POMDP} approach to personalize mammography screening decisions},
  author={Ayer, Turgay and Alagoz, Oguzhan and Stout, Natasha K},
  journal={Operations Research},
  volume={60},
  number={5},
  pages={1019--1034},
  year={2012},
  publisher={INFORMS}
}

@inproceedings{wang2018online,
  title={An Online Planner for {POMDP}s with Large Discrete Action Space: A Quantile-Based Approach.},
  author={Wang, Erli and Kurniawati, Hanna and Kroese, Dirk P.},
  booktitle=icaps,
  year={2018}
}

@article{papadimitriou1987complexity,
  title={The Complexity of {M}arkov Decision Processes},
  author={Papadimitriou, Christos H. and Tsitsiklis, John N.},
  journal={Mathematics of Operations Research},
  volume={12},
  number={3},
  pages={441--450},
  year={1987},
  publisher={{INFORMS}}
}

@inproceedings{kaebling1995learning,
  title={Learning Policies for Partially Observable Environments: Scaling Up},
  author={Kaebling, LP and Littman, ML and Cassandra, AR},
  booktitle={Proceedings of the Twelfth International Conference on Machine Learning},
  year={1995}
}

@inproceedings{kurniawati2008sarsop,
  title={{SARSOP}: Efficient Point-based {POMDP} Planning by Approximating Optimally Reachable Belief Spaces},
  author={Kurniawati, Hanna and Hsu, David and Lee, Wee Sun},
  booktitle=rss,
  volume={2008},
  year={2008},
}
