@String { aaai        = {AAAI Conference on Artificial Intelligence (AAAI)} }
@String { aamas       = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)} }
@String { acc         = {American Control Conference (ACC)} }
@String { atm         = {Air Traffic Management Research and Development Seminar}}
@String { aiaa_info   = {AIAA Infotech@Aerospace Conference} }
@String { aiaa_jacic  = {Journal of Aerospace Computing, Information, and Communication} }
@String { allerton    = {Allerton Conference on Communication, Control, and Compution} }
@String { atio        = {AIAA Aviation Technology, Integration, and Operations Conference (ATIO)} }
@String { cacm        = {Communications of the ACM} }
@String { cav         = {International Conference on Computer-Aided Verification} }
@String { cdc         = {IEEE Conference on Decision and Control (CDC)} }
@String { cvpr        = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)} }
@String { dasc        = {Digital Avionics Systems Conference (DASC)} }
@String { ecml        = {European Conference on Machine Learning (ECML)} }
@String { gnc         = {AIAA Guidance, Navigation, and Control Conference (GNC)} }
@String { icaart      = {International Conference on Agents and Artificial Intelligence (ICAART)} }
@String { icaps       = {International Conference on Automated Planning and Scheduling (ICAPS)} }
@String { icassp      = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)} }
@String { iclr        = {International Conference on Learning Representations (ICLR)} }
@String { icml        = {International Conference on Machine Learning (ICML)} }
@String { icmla       = {International Conference on Machine Learning and Applications (ICMLA)} }
@String { icra        = {IEEE International Conference on Robotics and Automation (ICRA)} }
@String { icslp       = {International Conference on Spoken Language Processing (ICSLP)} }
@String { ieee_csm    = {IEEE Control Systems Magazine} }
@String { ieee_tc     = {IEEE Transactions on Cybernetics} }
@String { ieee_j_ac   = {IEEE Transactions on Automatic Control} }
@String { ieeeaero    = {IEEE Aerospace Conference} }
@String { ieeeciaig   = {IEEE Transactions on Computational Intelligence and AI in Games} }
@String { ieeecst     = {IEEE Transactions on Control Systems Technology} }
@String { ieeetiv     = {IEEE Transactions on Intelligent Vehicles} }
@String { ieeetac     = {IEEE Transactions on Automatic Control} }
@String { ieeetits     = {IEEE Transactions on Intelligent Transportation Systems} }
@String { ieeetsp     = {IEEE Transactions on Signal Processing} }
@String { ijcai       = {International Joint Conference on Artificial Intelligence (IJCAI)} }
@String { ijrr        = {International Journal of Robotics Research} }
@String { interspeech = {Annual Conference of the International Speech Communication Association (INTERSPEECH)} }
@String { iros        = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)} }
@String { itsc        = {IEEE International Conference on Intelligent Transportation Systems (ITSC)} }
@String { iv          = {IEEE Intelligent Vehicles Symposium (IV)} }
@String { jaamas      = {Journal of Autonomous Agents and Multi-Agent Systems} }
@String { jair        = {Journal of Artificial Intelligence Research} }
@String { jgcd        = {AIAA Journal of Guidance, Control, and Dynamics} }
@String { jmlr        = {Journal of Machine Learning Research} }
@String { jota        = {Journal of Optimization Theory and Applications} }
@String { lion        = {Learning and Intelligent Optimization (LION)} }
@String { mor         = {Mathematics of Operations Research} }
@String { nips        = {Advances in Neural Information Processing Systems (NIPS)} }
@String { neurips     = {Advances in Neural Information Processing Systems (NeurIPS)} }
@String { or          = {Operations Research} }
@String { rss         = {Robotics: Science and Systems} }
@String { sigcomm     = {ACM Special Interest Group on Data Communication (SIGCOMM)} }
@String { tac         = {IEEE Transactions on Automatic Control} }
@String { tacas       = {International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS)} }
@String { taes        = {IEEE Transactions on Aerospace and Electronic Systems} }
@String { uai         = {Conference on Uncertainty in Artificial Intelligence (UAI)} }

@inproceedings{sunberg2018pomcpow,
  title={Online Algorithms for {POMDP}s with Continuous State, Action, and Observation Spaces},
  author={Zachary Sunberg and Mykel J. Kochenderfer},
  author+an={1=me},
  booktitle={International Conference on Automated Planning and Scheduling},
  year={2018}
}

@inproceedings{garg2019despotalpha,
  title={{DESPOT}-$\alpha$: Online {POMDP} Planning With Large State And Observation Spaces},
  author={Neha P. Garg and David Hsu and Wee Sun Lee},
  booktitle={Robotics: Science and Systems},
  year={2019}
}

@inproceedings{lee2020monte,
  title={{M}onte-{C}arlo Tree Search in Continuous Action Spaces with Value Gradients},
  author={Lee, Jongmin and Jeon, Wonseok and Kim, Geon-Hyeong and Kim, Kee-Eung},
  booktitle=aaai,
  pages={4561--4568},
  year={2020}
}

@inproceedings{kim2020monte,
  title={Monte Carlo Tree Search in Continuous Spaces Using Voronoi Optimistic Optimization with Regret Bounds.},
  author={Kim, Beomjoon and Lee, Kyungjae and Lim, Sungbin and Kaelbling, Leslie Pack and Lozano-P{\'e}rez, Tom{\'a}s},
  booktitle=aaai,
  pages={9916--9924},
  year={2020}
}

@inproceedings{lim2020sparse,
  title={Sparse Tree Search Optimality Guarantees in {POMDP}s with Continuous Observation Spaces},
  author={Lim, Michael H. and Tomlin, Claire J. and Sunberg, Zachary N.},
  author+an={3=me},
  year={2020},
  booktitle=ijcai
}

@inproceedings{garg2019learning,
  title={Learning To Grasp Under Uncertainty Using POMDPs},
  author={Garg, Neha P and Hsu, David and Lee, Wee Sun},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={2751--2757},
  year={2019},
  organization={IEEE}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@misc{darpa2020ace,
    title={Broad Agency Announcement: Air Combat Evolution Technical Area 1: Build Combat Autonomy},
    author={Defence Advanced Research Projects Agency (DARPA)},
    year={2020}
}

@article {jaderberg2019quake,
	author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Casta{\~n}eda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
	title = {Human-level performance in 3D multiplayer games with population-based reinforcement learning},
	volume = {364},
	number = {6443},
	pages = {859--865},
	year = {2019},
	doi = {10.1126/science.aau6249},
	publisher = {American Association for the Advancement of Science},
	abstract = {Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans.Science, this issue p. 859Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/364/6443/859},
	eprint = {https://science.sciencemag.org/content/364/6443/859.full.pdf},
	journal = {Science}
}

@Article{kearns2002sparse,
    author="Kearns, Michael and Mansour, Yishay and Ng, Andrew Y.",
    title="A Sparse Sampling Algorithm for Near-Optimal Planning in Large {M}arkov Decision Processes",
    journal="Machine Learning",
    year="2002",
    month="Nov",
    day="01",
    volume="49",
    number="2",
    pages="193--208",
    issn="1573-0565",
}

@article {silver2018alphago,
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	title = {A general reinforcement learning algorithm that masters {C}hess, {S}hogi, and {G}o through self-play},
	volume = {362},
	number = {6419},
	pages = {1140--1144},
	year = {2018},
	doi = {10.1126/science.aar6404},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.Science, this issue p. 1140; see also pp. 1087 and 1118The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/362/6419/1140},
	eprint = {https://science.sciencemag.org/content/362/6419/1140.full.pdf},
	journal = {Science}
}

@inproceedings{silver2010pomcp,
    title = {{M}onte-{C}arlo Planning in Large {POMDP}s},
    author = {Silver, David and Veness, Joel},
    booktitle = {Advances in Neural Information Processing Systems},
    pages = {2164--2172},
    year = {2010},
    url = {http://papers.nips.cc/paper/4031-monte-carlo-planning-in-large-pomdps.pdf}
}

@article{ye2017despot,
  title={{DESPOT}: Online {POMDP} Planning with Regularization},
  author={Ye, Nan and Somani, Adhiraj and Hsu, David and Lee, Wee Sun},
  journal={Journal of Artificial Intelligence Research},
  volume={58},
  pages={231--266},
  year={2017}
}

@inproceedings{seiler2015online,
  title={An online and approximate solver for {POMDP}s with continuous action space},
  author={Seiler, Konstantin M. and Kurniawati, Hanna and Singh, Surya P. N.},
  booktitle=icra,
  pages={2290--2297},
  year={2015},
}

@article{browne2012survey,
  title={A survey of {M}onte {C}arlo tree search methods},
  author={Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={{IEEE} Transactions on Computational Intelligence and {AI} in games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
}

@inproceedings{keller2013trial,
  title={Trial-based heuristic tree search for finite horizon MDPs},
  author={Keller, Thomas and Helmert, Malte},
  booktitle={Twenty-Third International Conference on Automated Planning and Scheduling},
  year={2013}
}

@inproceedings{sunberg2017value,
    title={The Value of Inferring the Internal State of Traffic Participants for Autonomous Freeway Driving},
    author={Sunberg, Zachary N. and Ho, Christopher J. and Kochenderfer, Mykel, J.},
    author+an={1=me},
    booktitle=acc,
    year={2017}
}

@inproceedings{couetoux2011double,
  address = {Rome, Italy},
  annote = {double progressive widening},
  author = {Cou\"{e}toux, A. and Hoock, J.-B. and Sokolovska, N. and Teytaud, O. and Bonnard, N.},
  booktitle = {Learning and Intelligent Optimization},
  mendeley-groups = {UAVCAS,POMDPs,MDPs},
  title = {Continuous Upper Confidence Trees},
  year = {2011}
}

@inproceedings{frank2007hover,
  title={Hover, transition, and level flight control design for a single-propeller indoor airplane},
  author={Frank, Adrian and McGrew, James and Valenti, Mario and Levine, Daniel and How, Jonathan},
  booktitle={AIAA Guidance, Navigation and Control Conference and Exhibit},
  pages={6318},
  year={2007}
}

@book{shaw1985fighter,
  title={Fighter Combat: Tactics and Maneuvering},
  author={Shaw, Robert L.},
  year={1985},
  publisher={Naval Institute Press},
  address={Annapolis}
}

@Article{holland2013optimizing,
  Title                    = {Optimizing the Next Generation Collision Avoidance System for Safe, Suitable, and Acceptable Operational Performance},
  Author                   = {Jessica E. Holland and Mykel J. Kochenderfer and Wesley A. Olson},
  Journal                  = {{A}ir {T}raffic {C}ontrol {Q}uarterly},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {275-297},
  Volume                   = {21}
}

@inproceedings{cassandra1998survey,
  title={A survey of {POMDP} applications},
  author={Cassandra, Anthony R},
  booktitle={Working notes of {AAAI} 1998 fall symposium on planning with partially observable {M}arkov decision processes},
  year={1998}
}

@article{ayer2012mammography,
  title={A {POMDP} approach to personalize mammography screening decisions},
  author={Ayer, Turgay and Alagoz, Oguzhan and Stout, Natasha K},
  journal={Operations Research},
  volume={60},
  number={5},
  pages={1019--1034},
  year={2012},
  publisher={INFORMS}
}

@inproceedings{wang2018online,
  title={An Online Planner for {POMDP}s with Large Discrete Action Space: A Quantile-Based Approach.},
  author={Wang, Erli and Kurniawati, Hanna and Kroese, Dirk P.},
  booktitle=icaps,
  year={2018}
}

@article{papadimitriou1987complexity,
  title={The Complexity of {M}arkov Decision Processes},
  author={Papadimitriou, Christos H. and Tsitsiklis, John N.},
  journal={Mathematics of Operations Research},
  volume={12},
  number={3},
  pages={441--450},
  year={1987},
  publisher={{INFORMS}}
}

@inproceedings{kaebling1995learning,
  title={Learning Policies for Partially Observable Environments: Scaling Up},
  author={Kaebling, LP and Littman, ML and Cassandra, AR},
  booktitle={Proceedings of the Twelfth International Conference on Machine Learning},
  year={1995}
}

@inproceedings{kurniawati2008sarsop,
  title={{SARSOP}: Efficient Point-based {POMDP} Planning by Approximating Optimally Reachable Belief Spaces},
  author={Kurniawati, Hanna and Hsu, David and Lee, Wee Sun},
  booktitle=rss,
  volume={2008},
  year={2008},
}


@techreport{young2020architecture,
  title={Architecture and Information Requirements to Assess and Predict Flight Safety Risks During Highly Autonomous Urban Flight Operations},
  author={Young, Steven and Ancel, Ersin and Moore, Andrew and Dill, Evan and Quach, Cuong and Foster, John and Darafsheh, Kaveh and Smalling, Kyle and Vazquez, Sixto and Evans, Emory and others},
  year={2020},
  institution={NASA},
  number={TM-2020-220440}
}

@book{nas2018aviation,
    title={In-Time Aviation Safety Mangement: Challenges and Research for an Evolving Aviation System},
    author={{National Academies of Sciences, Engineering, and Medicine}},
    publisher={The National Academies Press},
    year={2018},
    doi={10.17226/24962}
}

@book{nasa2019strategic,
    title={{NASA} Aeronautics Strategic Implementation Plan},
    author={{National Aeronautics and Space Administration}},
    year={2019}
}

@inproceedings{bouton2018utility,
  title={Utility Decomposition with Deep Corrections for Scalable Planning under Uncertainty},
  author={Bouton, Maxime and Julian, Kyle and Nakhaei, Alireza and Fujimura, Kikuo and Kochenderfer, Mykel J.},
  booktitle=aamas,
  pages={462--469},
  year={2018}
}

@inproceedings{bouton2020point,
  title={Point-Based Methods for Model Checking in Partially Observable Markov Decision Processes},
  author={Bouton, Maxime and Tumova, Jana and Kochenderfer, Mykel J.},
  booktitle=aaai,
  pages={10061--10068},
  year={2020}
}

@inproceedings{lee2015adaptive,
  title={Adaptive Stress Testing of Airborne Collision Avoidance Systems},
  author={Lee, Ritchie and Kochenderfer, Mykel J. and Mengshoel, Ole J. and Brat, Guillaume P. and Owen, Michael P.},
  booktitle=dasc,
  year={2015},
  organization={{IEEE}}
}

@article{best2019dec,
  title={Dec-{MCTS}: Decentralized Planning for Multi-Robot Active Perception},
  author={Best, Graeme and Cliff, Oliver M. and Patten, Timothy and Mettu, Ramgopal R. and Fitch, Robert},
  journal=ijrr,
  volume={38},
  number={2-3},
  pages={316--337},
  year={2019},
  publisher={SAGE Publications}
}

@techreport{knkt2018accident,
    author={{Komite Nasional Keselamatan Transportasi}},
    title={Aircraft Accident Investigation Report},
    year={2018},
    number={KNKT.18.10.35.04}
}

@unpublished{sunberg2021improving,
    author={Sunberg, Zachary and Kochenderfer, Mykel},
    author+an={1=me},
    title={Improving Automated Driving through Planning with Human Internal States},
    note={Under revision for {IEEE} Transactions on Intelligent Transportation Systems},
    url={https://arxiv.org/abs/2005.14549}
}

@article{egorov2017pomdps,
  author  = {Maxim Egorov and Zachary N. Sunberg and Edward Balaban and Tim A. Wheeler and Jayesh K. Gupta and Mykel J. Kochenderfer},
  title   = {{POMDP}s.jl: A Framework for Sequential Decision Making under Uncertainty},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {26},
  pages   = {1-5},
  url     = {http://jmlr.org/papers/v18/16-300.html}
}

@inproceedings{sunberg2016trusted,
    title={Optimized and Trusted Collision Avoidance for Unmanned Aerial Vehicles using Approximate Dynamic Programming},
    author={Sunberg, Zachary and Kochenderfer, Mykel J. and Pavone, Marco},
    author+an={1=me},
    booktitle=icra,
    address={Stockholm},
    url={https://arxiv.org/abs/1602.04762},
    url_Paper={https://arxiv.org/pdf/1602.04762.pdf},
    year={2016}
}

@article{sunberg2015real,
    title = "A Real-Time Expert Control System For Helicopter Autorotation",
    author = {Sunberg, Zachary N. and Miller, Nathaniel R. and Rogers, Jonathan D.},
    author+an={1=me},
    journal = "Journal of the American Helicopter Society",
    parent_itemid = "infobike://ahs/jahs",
    publishercode ="ahs",
    year = "2015",
    volume = "60",
    number = "2",
    pages = "1-15",
    itemtype = "ARTICLE",
    issn = "2161-6027",
    url = "http://www.ingentaconnect.com/content/ahs/jahs/2015/00000060/00000002/art00008",
    url_Paper = "http://www.ingentaconnect.com/content/ahs/jahs/2015/00000060/00000002/art00008",
    doi = "10.4050/JAHS.60.022008"
}

@inproceedings{peters2020alignment,
    author={Lasse Peters and David Fridovich-Keil and Claire Tomlin and Zachary Sunberg},
    author+an={4=me},
    title={Inference-Based Strategy Alignment for General-Sum Differential Games},
    booktitle=aamas,
    year=2020,
    url={https://github.com/lassepe/AAMAS2020-GameInference-Paper/blob/master/submission/ibsa-camera-ready-aamas2020.pdf},
    url_Paper={https://github.com/lassepe/AAMAS2020-GameInference-Paper/blob/master/submission/ibsa-camera-ready-aamas2020.pdf}
}

@article{slade2020estimation,
    author={Patrick Slade and Zachary Sunberg and Mykel J. Kochenderfer},
    author+an={2=me},
    title={Estimation and Control Using Sampling-Based {B}ayesian Reinforcement Learning},
    journal={{IET} Cyber-Physical Systems: Theory and Applications},
    year={2020},
    volume={5},
    issue={1},
    url={https://digital-library.theiet.org/content/journals/10.1049/iet-cps.2019.0045}
}

@inproceedings{sonu2018hierarchy,
    title={Exploiting Hierarchy for Scalable Decision Making in Autonomous Driving},
    author={Sonu, Ekhlas and Sunberg, Zachary and Kochenderfer, Mykel J.},
    author+an={2=me},
    booktitle={Intelligent Vehicles Symposium},
    year={2018},
    address={Changshu}
}

@article{sunberg2016information,
    title={Information Space Receding Horizon Control for Multisensor Tasking Problems},
    author={Sunberg, Zachary and Chakravorty, Suman and Erwin, Richard Scott},
    author+an={1=me},
    journal=ieee_tc,
    volume={46},
    number={6},
    pages={1325--1336},
    year={2016},
    url={http://ieeexplore.ieee.org/document/7174988/},
    url_Paper={http://ieeexplore.ieee.org/document/7174988/},
}

@article{sunberg2013belief,
    title = "A Belief Function Distance Metric for Orderable Sets",
    journal = "Information Fusion",
    volume = "14",
    number = "4",
    pages = "361--373",
    year = "2013",
    issn = "1566-2535",
    doi = "10.1016/J.INFFUS.2013.03.003",
    url = "http://www.sciencedirect.com/science/article/pii/S1566253513000304",
    author = "Zachary Sunberg and Jonathan Rogers",
    author+an={2=me},
    keywords = "Dempster–Shafer theory",
    keywords = "Distance metric",
    keywords = "Orderable sets",
    keywords = "Sensor fault detection"
}

@Book{kochenderfer2015decision,
  title =         {Decision Making Under Uncertainty: Theory and Application},
  publisher =     {{MIT} Press},
  year =          {2015},
  author =        {Mykel J. Kochenderfer}
}

@article{ross2008online,
  title={Online planning algorithms for {POMDP}s},
  author={Ross, St{\'e}phane and Pineau, Joelle and Paquet, S{\'e}bastien and Chaib-Draa, Brahim},
  journal={Journal of Artificial Intelligence Research},
  volume={32},
  pages={663--704},
  year={2008}
}

@report{bea2012airfrance,
  title={Final report on the accident on 1st June 2009 to the Airbus A330-203 registered F-GZCP operated by Air France flight AF 447 Rio de Janeiro--Paris},
  author={Bureau d’Enqu{\^e}tes et d’Analyses},
  year={2012}
}

@inproceedings{moore2018testing,
  title={Testing enabling technologies for safe {UAS} urban operations},
  author={Moore, Andrew and Balachandran, Swee and Young, Steven D and Dill, Evan T and Logan, Michael J and Glaab, Louis J and Munoz, Cesar and Consiglio, Maria},
  booktitle={2018 Aviation Technology, Integration, and Operations Conference},
  pages={3200},
  year={2018}
}

@inproceedings{consiglio2016icarous,
  title={{ICAROUS}: Integrated configurable algorithms for reliable operations of unmanned systems},
  author={Consiglio, Mar{\'\i}a and Munoz, C{\'e}sar and Hagen, George and Narkawicz, Anthony and Balachandran, Swee},
  booktitle=dasc,
  year={2016}
}

@book{bertsekas1995dynamic,
  title={Dynamic Programming and Optimal Control},
  author={Bertsekas, Dimitri P.},
  year={1995},
  publisher={Athena Scientific}
}

@book{koller2009probabilistic,
  title={Probabilistic Graphical Models: Principles and Techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009},
  publisher={MIT press}
}

@article{ferreiro2012application,
  title={Application of {B}ayesian Networks in Prognostics for a New Integrated Vehicle Health Management Concept},
  author={Ferreiro, Susana and Arnaiz, Aitor and Sierra, Basilio and Irigoien, Itziar},
  journal={Expert Systems with Applications},
  volume={39},
  number={7},
  pages={6402--6418},
  year={2012},
  publisher={Elsevier}
}

@article{xu2015data,
  title={Data mining--based intelligent fault diagnostics for integrated system health management to avionics},
  author={Xu, Jiuping and Sun, Kai and Xu, Lei},
  journal={Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability},
  volume={229},
  number={1},
  pages={3--15},
  year={2015},
  publisher={{SAGE} Publications Sage UK: London, England}
}

@inproceedings{fridovich2020efficient,
  title={Efficient Iterative Linear-quadratic Approximations for Nonlinear Multi-player General-sum Differential Games},
  author={Fridovich-Keil, David and Ratner, Ellis and Peters, Lasse and Dragan, Anca D and Tomlin, Claire J},
  booktitle=icra,
  pages={1475--1481},
  year={2020},
  organization={IEEE}
}

@inproceedings{mern2021bayesian,
  title={Bayesian Optimized {M}onte {C}arlo Planning}, 
  author={John Mern and Anil Yildiz and Zachary Sunberg and Tapan Mukerji and Mykel J. Kochenderfer},
  author+an={3=me},
  year={2021},
  booktitle=aaai
}

@misc{hoerger2020online,
      title={An On-Line POMDP Solver for Continuous Observation Spaces}, 
      author={Marcus Hoerger and Hanna Kurniawati},
      year={2020},
      eprint={2011.02076},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

%%article{vinyals2019grandmaster,
%  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
%  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
%  journal={Nature},
%  volume={575},
%  number={7782},
%  pages={350--354},
%  year={2019},
%  publisher={Nature Publishing Group}
%}

@article{vinyals2019grandmaster,
author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M.  and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H.  and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P.  and Jaderberg, Max and Vezhnevets, Alexander S.  and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L.  and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
journal={Nature},
year={2019},
month={Nov},
day={01},
volume={575},
number={7782},
pages={350-354},
abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
issn={1476-4687},
doi={10.1038/s41586-019-1724-z},
url={https://doi.org/10.1038/s41586-019-1724-z}
}

@misc{darpa2020alpha,
    title={{A}lpha{D}ogfight Trials Foreshadow Future of Human-Machine Symbiosis},
    url={https://www.darpa.mil/news-events/2020-08-26}
}

@article{openai2019dota,
  title={Dota 2 with Large Scale Deep Reinforcement Learning},
  author={OpenAI and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique Pondé de Oliveira Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
  year={2019},
  eprint={1912.06680},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/1912.06680}
}

@article{jaderberg2019human,
	author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Casta{\~n}eda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
	title = {Human-level Performance in {3D} Multiplayer Games with Population-based Reinforcement Learning},
	volume = {364},
	number = {6443},
	pages = {859--865},
	year = {2019},
	doi = {10.1126/science.aau6249},
	publisher = {American Association for the Advancement of Science},
	abstract = {Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans.Science, this issue p. 859Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/364/6443/859},
	eprint = {https://science.sciencemag.org/content/364/6443/859.full.pdf},
	journal = {Science}
}

@article{moravcik2017deepstack,
	author = {Morav{\v c}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\'y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
	title = {{D}eep{S}tack: Expert-level Artificial Intelligence in Heads-up No-limit Poker},
	volume = {356},
	number = {6337},
	pages = {508--513},
	year = {2017},
	doi = {10.1126/science.aam6960},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers can beat humans at games as complex as chess or go. In these and similar games, both players have access to the same information, as displayed on the board. Although computers have the ultimate poker face, it has been tricky to teach them to be good at poker, where players cannot see their opponents{\textquoteright} cards. Morav{\v c}{\'\i}k et al. built a code dubbed DeepStack that managed to beat professional poker players at a two-player poker variant called heads-up no-limit Texas hold{\textquoteright}em. Instead of devising its strategy beforehand, DeepStack recalculated it at each step, taking into account the current state of the game. The principles behind DeepStack may enable advances in solving real-world problems that involve information asymmetry.Science, this issue p. 508Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold{\textquoteright}em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/356/6337/508},
	eprint = {https://science.sciencemag.org/content/356/6337/508.full.pdf},
	journal = {Science}
}

@article{brown2018superhuman,
	author = {Brown, Noam and Sandholm, Tuomas},
	title = {Superhuman AI for Heads-up No-limit Poker: Libratus Beats Top Professionals},
	volume = {359},
	number = {6374},
	pages = {418--424},
	year = {2018},
	doi = {10.1126/science.aao1733},
	publisher = {American Association for the Advancement of Science},
	abstract = {Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold{\textquoteright}em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent.Science, this issue p. 418No-limit Texas hold{\textquoteright}em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold{\textquoteright}em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/359/6374/418},
	eprint = {https://science.sciencemag.org/content/359/6374/418.full.pdf},
	journal = {Science}
}

@unpublished{lim2021voronoi,
      title={Voronoi Progressive Widening: Efficient Online Solvers for Continuous Space {MDP}s and {POMDP}s with Provably Optimal Components}, 
      author={Michael H. Lim and Claire J. Tomlin and Zachary N. Sunberg},
      year={2021},
      booktitle=icaps,
      note={Under Review for ICAPS 2021}
}

@book{sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  year={2018},
  publisher={MIT press}
}

@inproceedings{mania2019certainty,
 author = {Mania, Horia and Tu, Stephen and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Certainty Equivalence is Efficient for Linear Quadratic Control},
 url = {https://proceedings.neurips.cc/paper/2019/file/5dbc8390f17e019d300d5a162c3ce3bc-Paper.pdf},
 volume = {32},
 year = {2019}
}

@book{kochenderfer2019algorithms,
  title={Algorithms for Optimization},
  author={Kochenderfer, Mykel J. and Wheeler, Tim A.},
  year={2019},
  publisher={{MIT} Press}
}

@article{cai2020hyp,
  title={{HyP-DESPOT}: A Hybrid Parallel Algorithm for Online Planning under Uncertainty},
  author={Cai, Panpan and Luo, Yuanfu and Hsu, David and Lee, Wee Sun},
  journal={The International Journal of Robotics Research},
  pages={0278364920937074},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{lee2020magic,
  title={{MAGIC}: Learning Macro-Actions for Online {POMDP} Planning using Generator-Critic},
  author={Lee, Yiyuan and Cai, Panpan and Hsu, David},
  journal={arXiv preprint arXiv:2011.03813},
  year={2020}
}

@article{luo2018importance,
author = {Yuanfu Luo and Haoyu Bai and David Hsu and Wee Sun Lee},
title ={Importance sampling for online planning under uncertainty},
journal = {The International Journal of Robotics Research},
volume = {38},
number = {2-3},
pages = {162-181},
year = {2019},
doi = {10.1177/0278364918780322},

URL = { 
        https://doi.org/10.1177/0278364918780322
    
},
eprint = { 
        https://doi.org/10.1177/0278364918780322
    
}
,
    abstract = { The partially observable Markov decision process (POMDP) provides a principled general framework for robot planning under uncertainty. Leveraging the idea of Monte Carlo sampling, recent POMDP planning algorithms have scaled up to various challenging robotic tasks, including, real-time online planning for autonomous vehicles. To further improve online planning performance, this paper presents IS-DESPOT, which introduces importance sampling to DESPOT, a state-of-the-art sampling-based POMDP algorithm for planning under uncertainty. Importance sampling improves DESPOT’s performance when there are critical, but rare events, which are difficult to sample. We prove that IS-DESPOT retains the theoretical guarantee of DESPOT. We demonstrate empirically that importance sampling significantly improves the performance of online POMDP planning for suitable tasks. We also present a general method for learning the importance sampling distribution. }
}

@article{kaelbling1998planning,
  title={Planning and Acting in Partially Observable Stochastic Domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={101},
  number={1-2},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}

@inproceedings{araya2010pomdp,
  title={A {POMDP} Extension with Belief-dependent Rewards},
  author={Araya-L{\'o}pez, Mauricio and Buffet, Olivier and Thomas, Vincent and Charpillet, Fran{\c{c}}ois},
  booktitle=nips,
  year={2010},
  organization={MIT Press}
}

@inproceedings{bouton2018scalable,
  title={Scalable Decision Making with Sensor Occlusions for Autonomous Driving},
  author={Bouton, Maxime and Nakhaei, Alireza and Fujimura, Kikuo and Kochenderfer, Mykel J},
  booktitle=icra,
  pages={2076--2081},
  year={2018},
  organization={IEEE}
}

@inproceedings{thornton2018value,
  title={Value Sensitive Design for Autonomous Vehicle Motion Planning},
  author={Thornton, Sarah M and Lewis, Francis E and Zhang, Vivian and Kochenderfer, Mykel J and Gerdes, J Christian},
  booktitle={{IEEE} Intelligent Vehicles Symposium (IV)},
  pages={1157--1162},
  year={2018},
  organization={IEEE}
}

@article{akbarinasaji2020partially,
title = {Partially observable Markov decision process to generate policies in software defect management},
journal = {Journal of Systems and Software},
volume = {163},
pages = {110518},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110518},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300017},
author = {Shirin Akbarinasaji and Can Kavaklioglu and Ayşe Başar and Adam Neal},
keywords = {Defect management, Policy, Reinforcement learning, Partially observable Markov decision Process, Partially observable Monte Carlo Planning},
abstract = {Bug repositories are dynamic in nature and as new bugs arrive, the old ones are closed. In a typical software project, bugs and their dependencies are reported manually and gradually using a issue tracking system. Thus, not all of the bugs in the system are available at any time, creating uncertainty in the dependency structure of the bugs. In this research, we propose to construct a dependency graph based on the reported dependency-blocking information in a issue tracking system. We use two graph metrics, depth and degree, to measure the extent of blocking bugs. Due to the uncertainty in the dependency structure, simply ordering bugs in the descending order of depth and/or degree may not be the best policy to prioritize bugs. Instead, we propose a Partially Observable Markov Decision Process model for sequential decision making and Partially Observable Monte Carlo Planning to identify the best policy for this sequential decision-making process. We validated our proposed approach by mining the data from two open source projects, and a commercial project. We compared our proposed framework with three baseline policies. The results on all datasets show that our proposed model significantly outperforms the other policies with respect to average discounted return.}
}

@INPROCEEDINGS{tomin2019intelligent,
  author={Tomin, Nikita and Kurbatsky, Victor and Guliyev, Huseyngulu},
  booktitle={Conference on Electrical Machines, Drives and Power Systems ({ELMA})}, 
  title={Intelligent Control of a Wind Turbine based on Reinforcement Learning}, 
  year={2019},
  pages={1-6},
  doi={10.1109/ELMA.2019.8771645}
}

@inproceedings{linares2016dynamic,
  title={Dynamic sensor tasking for space situational awareness via reinforcement learning},
  author={Linares, Richard and Furfaro, Roberto},
  booktitle={Advanced Maui Optical and Space Surveillance Tech. Conf.(AMOS)},
  year={2016}
}
